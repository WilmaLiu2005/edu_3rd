"""
ç‰¹å¾å·¥å…·æ¨¡å—

æä¾›ç‰¹å¾å¤„ç†ç›¸å…³çš„å·¥å…·å‡½æ•°ï¼ŒåŒ…æ‹¬ï¼š
- æ— ç©·å¤§å€¼å¤„ç†
- å…³é”®è¯æ£€æµ‹
- æ–‡æœ¬è§„èŒƒåŒ–
"""

from typing import List, Optional
import re
import numpy as np
import pandas as pd

# å¸¸é‡å®šä¹‰
COPY_KEYWORDS = [
    "å¦‚ä¸‹", "å¦‚ä¸Š", "è¿™é“é¢˜", "æ€ä¹ˆåš", "åšæ³•",
    "A.", "B.", "C.", "D.", "æˆ‘å¯¹è¿™ä¸€é¡µä¸æ‡‚",
    # æ–°å¢å…³é”®è¯
    "åšä¸€ä¸‹", "æ€ä¹ˆå†™", "é€‰ä»€ä¹ˆ", "è§£ä¸€ä¸‹", "å’‹åš", "ç­”æ¡ˆ", "ç»“æœ", "æ±‚è§£",
    "å®Œæˆ", "è§£ç­”", "å›ç­”", "è§£é¢˜", "è§£å†³", "ç¬¬", "é¢˜", "å•é€‰é¢˜", "å¤šé€‰é¢˜",
    "æè¿°é”™è¯¯çš„æ˜¯", "æè¿°æ­£ç¡®çš„æ˜¯", "å›ç­”ä¸‹åˆ—é—®é¢˜", "a.", "b.", "c.", "d.",
    "æœ‰ä»€ä¹ˆä½œç”¨"
]

# éœ€è¦æ­£åˆ™åŒ¹é…çš„å…³é”®è¯æ¨¡å¼
COPY_KEYWORDS_PATTERNS = [
    r'\d+åˆ†',  # æ•°å­—+åˆ†ï¼Œå¦‚"5åˆ†"ã€"10åˆ†"
]

DEFAULT_REPLACEMENT_VALUE = 168.0  # é»˜è®¤æ›¿æ¢å€¼ï¼ˆä¸€å‘¨çš„å°æ—¶æ•°ï¼‰

# ç‰¹å¾åˆ—åˆ—è¡¨
FEATURE_COLUMNS = [
    'qa_turns', 'is_multi_turn', 'total_time_minutes', 'avg_qa_time_minutes',
    'total_question_chars', 'avg_question_length',
    'if_non_class', 'is_video_unit', 'is_discussion_unit', 'is_graphic_unit', 'is_ai_task', 'is_confusion_entry',
    'avg_hours_to_assignment', 'avg_hours_since_release',
    'course_progress_ratio', 'calendar_week_since_2025_0217',
    'hours_to_next_class', 'hours_from_last_class', 'is_copy_paste', 'copy_keywords_count',
    'is_exam_week', 'day_period', 'is_weekend',
    'is_in_class_time', 'question_type_why_how'
]


def replace_inf_with_reasonable_value(
    series: pd.Series,
    multiplier: float = 1.5
) -> pd.Series:
    """
    å°†æ— ç©·å¤§å€¼æ›¿æ¢ä¸ºåˆç†çš„æœ‰é™å€¼
    
    Args:
        series: å¾…å¤„ç†çš„æ•°æ®åºåˆ—
        multiplier: ä¹˜æ•°å› å­ï¼ˆå½“å‰æœªä½¿ç”¨ï¼Œä¿ç•™ç”¨äºæœªæ¥æ‰©å±•ï¼‰
    
    Returns:
        pd.Series: æ›¿æ¢åçš„æ•°æ®åºåˆ—
    """
    if series.empty:
        return series
    
    # ä½¿ç”¨é»˜è®¤å€¼æ›¿æ¢æ— ç©·å¤§
    series_cleaned = series.replace([np.inf, -np.inf], DEFAULT_REPLACEMENT_VALUE)
    
    return series_cleaned


def debug_infinite_values(features_df: pd.DataFrame) -> None:
    """
    è°ƒè¯•å¹¶æ‰“å°æ•°æ®ä¸­çš„æ— ç©·å¤§å€¼å’ŒNaNå€¼ä¿¡æ¯
    
    Args:
        features_df: ç‰¹å¾DataFrame
    """
    print("\n" + "=" * 50)
    print("Checking Infinite Values")
    print("=" * 50)
    
    print("\nAvailable columns in features_df:")
    print(features_df.columns.tolist())
    
    has_issues = False
    
    for col in FEATURE_COLUMNS:
        if (col not in features_df.columns):
            continue
        
        inf_count = np.isinf(features_df[col]).sum()
        nan_count = np.isnan(features_df[col]).sum()
        
        if (inf_count > 0 or nan_count > 0):
            has_issues = True
            print(f"\nâš ï¸ Column '{col}':")
            print(f"   - Infinite values: {inf_count}")
            print(f"   - NaN values: {nan_count}")
            
            if (inf_count > 0):
                inf_indices = features_df[np.isinf(features_df[col])].index
                print(f"   - Infinite values in rows: {inf_indices.tolist()[:5]}...")
                
                if ('file_name' in features_df.columns):
                    sample_files = features_df.loc[inf_indices[:3], 'file_name'].tolist()
                    print(f"   - Sample files: {sample_files}")
        
        # æ˜¾ç¤ºåŸºæœ¬ç»Ÿè®¡ï¼ˆæ’é™¤æ— ç©·å¤§å€¼ï¼‰
        finite_data = features_df[col][np.isfinite(features_df[col])]
        if (not finite_data.empty):
            print(f"\nğŸ“Š {col} (finite values only):")
            print(f"   - Min:  {finite_data.min():.2f}")
            print(f"   - Max:  {finite_data.max():.2f}")
            print(f"   - Mean: {finite_data.mean():.2f}")
            print(f"   - Std:  {finite_data.std():.2f}")
        else:
            print(f"\nâŒ {col}: All values are infinite or NaN")
    
    if (not has_issues):
        print("\nâœ… No infinite or NaN values found in feature columns")
    
    print("=" * 50)


def normalize_for_keyword(text: str) -> str:
    """
    è§„èŒƒåŒ–æ–‡æœ¬ç”¨äºå…³é”®è¯æ£€æµ‹
    
    å¤„ç†æ­¥éª¤ï¼š
    1. å»é™¤Markdowné“¾æ¥ï¼Œä¿ç•™å¯è§æ–‡æœ¬
    2. å…¨è§’å­—ç¬¦è½¬åŠè§’
    3. å»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦
    
    Args:
        text: å¾…è§„èŒƒåŒ–çš„æ–‡æœ¬
    
    Returns:
        str: è§„èŒƒåŒ–åçš„æ–‡æœ¬
    """
    if (text is None):
        return ""
    
    s = str(text)
    
    # å»é™¤Markdowné“¾æ¥ï¼Œåªä¿ç•™å¯è§æ–‡æœ¬
    # s = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', s)
    
    # å…¨è§’è½¬åŠè§’
    result = []
    for ch in s:
        code = ord(ch)
        if (code == 0x3000):  # å…¨è§’ç©ºæ ¼
            code = 32
        elif (0xFF01 <= code <= 0xFF5E):  # å…¶ä»–å…¨è§’å­—ç¬¦
            code -= 0xFEE0
        result.append(chr(code))
    s = "".join(result)
    
    # å»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦ï¼ˆé¿å…"è¿™ é“ é¢˜"è¢«æ¼æ£€ï¼‰
    s = re.sub(r'\s+', '', s)
    
    return s


def contains_copy_keywords(
    text: str,
    keywords: List[str] = COPY_KEYWORDS
) -> bool:
    """
    æ£€æµ‹æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«å¤åˆ¶ç²˜è´´ç›¸å…³çš„å…³é”®è¯
    
    Args:
        text: å¾…æ£€æµ‹çš„æ–‡æœ¬
        is_confusion_entry: æ˜¯å¦ä¸ºå›°æƒ‘ç±»å…¥å£ (0=å¦, 1=æ˜¯)
        keywords: å…³é”®è¯åˆ—è¡¨
    
    Returns:
        bool: æ˜¯å¦åŒ…å«å…³é”®è¯
    
    è§„åˆ™:
        - æ™®é€šå…³é”®è¯: æ€»æ˜¯æ£€æµ‹
        - æ­£åˆ™æ¨¡å¼(å¦‚"\d+åˆ†"): æ€»æ˜¯æ£€æµ‹
        - å›¾ç‰‡ä¸Šä¼ : éœ€åœ¨è°ƒç”¨æ–¹å•ç‹¬å¤„ç†ï¼Œä»…åœ¨ is_confusion_entry==0 æ—¶ç”Ÿæ•ˆ
    """
    normalized_text = normalize_for_keyword(text)
    
    if not normalized_text:
        return False
    
    # æ£€æŸ¥æ™®é€šå…³é”®è¯
    if any(keyword in normalized_text for keyword in keywords):
        return True
    
    # æ£€æŸ¥æ­£åˆ™æ¨¡å¼å…³é”®è¯ï¼ˆæ€»æ˜¯æ£€æµ‹ï¼‰
    for pattern in COPY_KEYWORDS_PATTERNS:
        if re.search(pattern, normalized_text):
            return True
    
    return False


def count_copy_keywords(
    text: str,
    keywords: List[str] = COPY_KEYWORDS
) -> int:
    """
    ç»Ÿè®¡æ–‡æœ¬ä¸­åŒ…å«çš„å…³é”®è¯æ•°é‡
    
    Args:
        text: å¾…æ£€æµ‹çš„æ–‡æœ¬
        keywords: å…³é”®è¯åˆ—è¡¨
    
    Returns:
        int: å…³é”®è¯å‡ºç°æ¬¡æ•°
    """
    normalized_text = normalize_for_keyword(text)
    
    if not normalized_text:
        return 0
    
    # ç»Ÿè®¡æ™®é€šå…³é”®è¯
    count = sum(1 for keyword in keywords if keyword in normalized_text)
    
    # ç»Ÿè®¡æ­£åˆ™æ¨¡å¼å…³é”®è¯
    for pattern in COPY_KEYWORDS_PATTERNS:
        matches = re.findall(pattern, normalized_text)
        count += len(matches)
    
    return count