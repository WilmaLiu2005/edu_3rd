# å…ˆæ—¶é—´é—´éš”15åˆ†é’Ÿï¼Œç„¶åå†LLMåˆ’åˆ†
import pandas as pd
import numpy as np
import os
import json
import json5
import re
import time
import random
from datetime import datetime
from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed
import csv
# ==============================    

MAX_WORKERS = 15
MAX_RETRIES = 3
INITIAL_RETRY_DELAY = 1
MAX_RETRY_DELAY = 3
API_KEY = "sk-0jErqj61bIYM135CEqhfj318rKIM1TIa"  # å¡«å†™ä½ çš„ API key
BASE_URL = "https://api-gateway.glm.ai/v1"
MODEL_NAME = "gemini-2.5-flash"  # æˆ–ä½ è‡ªå·±çš„æ¨¡å‹
INPUT_FOLDER = "/Users/vince/undergraduate/KEG/edu/å­¦å ‚åœ¨çº¿æ•°æ®3rd/processed_split_field_output"   # è¾“å…¥ CSV æ–‡ä»¶å¤¹
OUTPUT_BASE_FOLDER = "/Users/vince/undergraduate/KEG/edu/å­¦å ‚åœ¨çº¿æ•°æ®3rd/split" # è¾“å‡º CSV æ–‡ä»¶å¤¹

# ==============================
# å·¥å…·å‡½æ•°
# ==============================
os.makedirs(OUTPUT_BASE_FOLDER, exist_ok=True)

def robust_read_csv(file_path, text_columns=None):
    """
    å®‰å…¨è¯»å– CSV æ–‡ä»¶ï¼Œé€‚ç”¨äºå«æœ‰æ¢è¡Œç¬¦ã€Markdown å›¾ç‰‡é“¾æ¥ã€åŒå¼•å·ç­‰æƒ…å†µã€‚
    """
    try:
        df = pd.read_csv(
            file_path,
            encoding="utf-8-sig",
            engine="python",
            quotechar='"',
            doublequote=True,
            keep_default_na=False,
        )
    except Exception as e:
        print(f"âš ï¸ è¯»å– CSV å¤±è´¥: {file_path}, é”™è¯¯: {e}")
        return None
    
    if df.empty:
        print(f"âš ï¸ æ–‡ä»¶ {file_path} æ˜¯ç©ºçš„")
        return None
    
    # å»é™¤åˆ—åå‰åç©ºæ ¼
    df.columns = df.columns.str.strip()
    
    # æ£€æŸ¥å¿…è¦åˆ—
    required_cols = ["å­¦ç”ŸID", "æé—®æ—¶é—´"]
    for col in required_cols:
        if col not in df.columns:
            print(f"âš ï¸ æ–‡ä»¶ {file_path} ç¼ºå°‘å¿…è¦åˆ—: {col}")
            return None
    
    # å¤„ç†æ–‡æœ¬åˆ—æ¢è¡Œç¬¦
    if text_columns:
        for col in text_columns:
            if col in df.columns:
                df[col] = df[col].astype(str).str.replace("\n", " ", regex=False)
    
    # è½¬æ¢æ—¶é—´åˆ—
    df["æé—®æ—¶é—´"] = pd.to_datetime(df["æé—®æ—¶é—´"], errors="coerce")
    df = df.dropna(subset=["æé—®æ—¶é—´"])
    
    if df.empty:
        print(f"âš ï¸ æ–‡ä»¶ {file_path} å…¨éƒ¨è¡Œæ— æ•ˆï¼ˆæé—®æ—¶é—´è§£æå¤±è´¥ï¼‰")
        return None
    
    return df

# ==============================
# GPT API è°ƒç”¨
# ==============================
def gpt_api_call(messages, model=MODEL_NAME):
    client = OpenAI(api_key=API_KEY, base_url=BASE_URL)
    for attempt in range(MAX_RETRIES):
        try:
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                max_completion_tokens=10000
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"Attempt {attempt + 1} failed: {str(e)}")
            if attempt < MAX_RETRIES - 1:
                delay = min(INITIAL_RETRY_DELAY * (2 ** attempt) + random.uniform(0, 1), MAX_RETRY_DELAY)
                time.sleep(delay)
            else:
                return None

# ==============================
# JSON è§£æï¼ˆå¢å¼ºé²æ£’æ€§ï¼‰
# ==============================
def robust_json_parse(text):
    """
    é²æ£’çš„JSONè§£æï¼Œèƒ½å¤„ç†å„ç§æ ¼å¼çš„è¿”å›
    """
    if not text:
        return {}
    
    # æ¸…ç†æ–‡æœ¬
    cleaned = re.sub(r"^```(?:json)?|```$", "", text.strip(), flags=re.MULTILINE).strip()
    
    try:
        result = json.loads(cleaned)
    except Exception:
        try:
            result = json5.loads(cleaned)
        except Exception:
            print(f"    âš ï¸ JSONè§£æå¤±è´¥ï¼ŒåŸå§‹è¾“å‡º: {text[:200]}")
            return {}
    
    # å¤„ç†ä¸åŒæ ¼å¼çš„è¿”å›
    if isinstance(result, dict):
        return result
    elif isinstance(result, list):
        # å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼ [1, 1, 2, 2, 3]ï¼Œè½¬æ¢ä¸ºå­—å…¸
        print(f"    âš ï¸ LLMè¿”å›äº†åˆ—è¡¨æ ¼å¼ï¼Œå°è¯•è½¬æ¢")
        converted = {}
        for idx, session_id in enumerate(result, 1):
            converted[str(idx)] = session_id
        return converted
    else:
        print(f"    âš ï¸ æœªçŸ¥çš„è¿”å›æ ¼å¼: {type(result)}")
        return {}
        
PROMPT_TEMPLATE = """
You are a dialogue segmentation expert. Analyze the following student-AI tutor interaction logs and segment them into coherent dialogue sessions.

Input: Chronologically ordered interactions with:
- Row number
- Timestamp  
- Student question
- AI response

Segmentation criteria:
1. Preserve chronological order strictly
2. Split when time gap exceeds reasonable conversation pause
3. Split when topic shifts significantly
4. Keep related exchanges in same session

Output: JSON mapping{{row_number: session_id}}, session_id starting from 1
No additional text or explanation.

Interaction logs:
{dialogues_json}
"""
# ==============================
# Stage 0: æé—®å…¥å£åˆ’åˆ†ï¼ˆæ–°å¢ï¼‰
# ==============================
def entrance_based_split(df):
    """
    æ ¹æ®æé—®å…¥å£çš„å˜åŒ–è¿›è¡Œåˆ’åˆ†
    å½“æé—®å…¥å£å‘ç”Ÿå˜åŒ–æ—¶ï¼Œè®¤ä¸ºæ˜¯ä¸åŒçš„å¯¹è¯åœºæ™¯
    """
    results = []
    if df.empty:
        return results
    
    # ç¡®ä¿æŒ‰æ—¶é—´æ’åº
    df = df.sort_values("æé—®æ—¶é—´").reset_index(drop=True)
    
    current_chunk = [df.iloc[0]]
    current_entrance = df.iloc[0].get("æé—®å…¥å£", "")
    
    for i in range(1, len(df)):
        row_entrance = df.iloc[i].get("æé—®å…¥å£", "")
        
        # å¦‚æœæé—®å…¥å£å‘ç”Ÿå˜åŒ–ï¼Œåˆ™åˆ†å‰²
        if row_entrance != current_entrance:
            results.append(pd.DataFrame(current_chunk))
            current_chunk = [df.iloc[i]]
            current_entrance = row_entrance
        else:
            current_chunk.append(df.iloc[i])
    
    # æ·»åŠ æœ€åä¸€ä¸ªchunk
    if current_chunk:
        results.append(pd.DataFrame(current_chunk))
    
    print(f"  - æŒ‰æé—®å…¥å£åˆ’åˆ†ä¸º {len(results)} ä¸ªç‰‡æ®µ")
    for idx, chunk in enumerate(results, 1):
        entrance = chunk.iloc[0].get("æé—®å…¥å£", "æœªçŸ¥")
        print(f"    ç‰‡æ®µ{idx}: å…¥å£={entrance}, è®°å½•æ•°={len(chunk)}")
    
    return results

# ==============================
# Stage 1: æ—¶é—´åˆ’åˆ†ï¼ˆä¿®æ”¹åï¼‰
# ==============================
def time_based_split(df, time_threshold=15):
    """
    åœ¨åŒä¸€ä¸ªæé—®å…¥å£å†…ï¼Œæ ¹æ®æ—¶é—´é—´éš”è¿›è¡Œåˆ’åˆ†
    """
    results = []
    if df.empty:
        return results
    
    current_chunk = [df.iloc[0]]
    for i in range(1, len(df)):
        delta = (df.iloc[i]["æé—®æ—¶é—´"] - df.iloc[i - 1]["æé—®æ—¶é—´"]).total_seconds() / 60
        if delta > time_threshold:
            results.append(pd.DataFrame(current_chunk))
            current_chunk = [df.iloc[i]]
        else:
            current_chunk.append(df.iloc[i])
    
    if current_chunk:
        results.append(pd.DataFrame(current_chunk))
    
    return results

# ==============================
# Stage 2: LLM åˆ’åˆ†
# ==============================
def llm_split(group_df):
    """ä½¿ç”¨LLMå¯¹å¯¹è¯è¿›è¡Œç»†åˆ†"""
    try:
        dialogues = []
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—
        required_columns = ['æé—®æ—¶é—´', 'æé—®å†…å®¹', 'AIå›å¤']
        missing_columns = [col for col in required_columns if col not in group_df.columns]
        
        if missing_columns:
            print(f"    âš ï¸ ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}ï¼Œè·³è¿‡LLMåˆ’åˆ†")
            return {}
        
        # æ„å»ºå¯¹è¯æ•°æ®
        for idx in range(len(group_df)):
            row = group_df.iloc[idx]
            dialogues.append({
                "row_number": idx + 1,
                "timestamp": str(row["æé—®æ—¶é—´"]),
                "question": str(row["æé—®å†…å®¹"])[:500],
                "ai_response": str(row["AIå›å¤"])[:500]
            })
        
        dialogues_json = json.dumps(dialogues, ensure_ascii=False, indent=2)
        
        # è°ƒç”¨LLM
        prompt = PROMPT_TEMPLATE.format(dialogues_json=dialogues_json)
        
        messages = [
            {"role": "system", "content": "You are a dialogue segmentation expert. Always output JSON dictionary format, never array."},
            {"role": "user", "content": prompt}
        ]
        
        response = gpt_api_call(messages)
        
        if not response:
            print("    âš ï¸ LLMæ— å“åº”")
            return {}
        
        # è§£æç»“æœ
        parsed_result = robust_json_parse(response)
        
        # å†æ¬¡éªŒè¯æ˜¯å¦ä¸ºå­—å…¸
        if not isinstance(parsed_result, dict):
            print(f"    âš ï¸ è§£æåä»ä¸æ˜¯å­—å…¸æ ¼å¼: {type(parsed_result)}")
            return {}
        
        # ç¡®ä¿æ‰€æœ‰é”®éƒ½æ˜¯å­—ç¬¦ä¸²ï¼Œå€¼éƒ½æ˜¯æ•´æ•°
        result = {}
        for key, value in parsed_result.items():
            try:
                result[str(key)] = int(value)
            except (ValueError, TypeError) as e:
                print(f"    âš ï¸ è½¬æ¢é”®å€¼å¯¹å¤±è´¥: {key}={value}, é”™è¯¯: {e}")
                continue
        
        return result
        
    except Exception as e:
        print(f"    âŒ LLMåˆ’åˆ†å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        return {}

# ==============================
# å¤„ç†å•ä¸ªå­¦ç”Ÿçš„æ•°æ®ï¼ˆå¢å¼ºé”™è¯¯å¤„ç†ï¼‰
# ==============================
def process_student_data(student_df, student_id, output_folder):
    """å¤„ç†å•ä¸ªå­¦ç”Ÿçš„æ‰€æœ‰å¯¹è¯æ•°æ®"""
    # æŒ‰æ—¶é—´æ’åº
    student_df = student_df.sort_values("æé—®æ—¶é—´").reset_index(drop=True)
    
    print(f"  å¼€å§‹å¤„ç†å­¦ç”Ÿ {student_id} çš„æ•°æ®...")
    print(f"  - æ€»è®°å½•æ•°: {len(student_df)}")
    
    # Stage 0: å…ˆæŒ‰æé—®å…¥å£åˆ’åˆ†ï¼ˆå¦‚æœæœ‰è¿™ä¸ªåˆ—ï¼‰
    if "æé—®å…¥å£" in student_df.columns:
        entrance_splits = entrance_based_split(student_df)
    else:
        print("  - æ²¡æœ‰'æé—®å…¥å£'åˆ—ï¼Œè·³è¿‡å…¥å£åˆ’åˆ†")
        entrance_splits = [student_df]
    
    file_index = 1  # è¯¥å­¦ç”Ÿçš„å¯¹è¯ç¼–å·
    
    # å¯¹æ¯ä¸ªå…¥å£ç‰‡æ®µè¿›è¡Œåç»­å¤„ç†
    for entrance_idx, entrance_group in enumerate(entrance_splits, start=1):
        if "æé—®å…¥å£" in entrance_group.columns:
            entrance_name = entrance_group.iloc[0].get("æé—®å…¥å£", "æœªçŸ¥")
            print(f"\n  å¤„ç†å…¥å£ç‰‡æ®µ {entrance_idx}/{len(entrance_splits)}: {entrance_name}")
        
        # Stage 1: æ—¶é—´é—´éš”åˆ’åˆ†
        time_splits = time_based_split(entrance_group)
        print(f"    - æ—¶é—´åˆ’åˆ†ä¸º {len(time_splits)} ä¸ªå­ç‰‡æ®µ")
        
        # Stage 2: LLM åˆ’åˆ†
        for time_idx, group in enumerate(time_splits, start=1):
            # é‡ç½®ç´¢å¼•
            group = group.reset_index(drop=True)
            
            # å¦‚æœç‰‡æ®µå¤ªå°ï¼Œç›´æ¥ä¿å­˜
            if len(group) <= 2:
                output_csv = os.path.join(output_folder, f"{student_id}_{file_index}.csv")
                group.to_csv(output_csv, index=False, encoding="utf-8", quoting=csv.QUOTE_ALL)
                print(f"    âœ… ç‰‡æ®µè¾ƒå°ï¼Œç›´æ¥ä¿å­˜: {student_id}_{file_index}.csv, å…± {len(group)} è¡Œ")
                file_index += 1
                continue
            
            # ä½¿ç”¨LLMè¿›è¡Œæ›´ç»†è‡´çš„åˆ’åˆ†
            mapping = llm_split(group)
            
            # æ£€æŸ¥mappingæ˜¯å¦æœ‰æ•ˆ
            if not mapping or not isinstance(mapping, dict):
                # LLMåˆ’åˆ†å¤±è´¥æˆ–è¿”å›æ ¼å¼é”™è¯¯ï¼Œæ•´ä¸ªæ—¶é—´ç‰‡ä½œä¸ºä¸€ä¸ªå¯¹è¯ä¿å­˜
                print(f"    âš ï¸ LLMåˆ’åˆ†æ— æ•ˆï¼Œæ•´ä½“ä¿å­˜")
                output_csv = os.path.join(output_folder, f"{student_id}_{file_index}.csv")
                group.to_csv(output_csv, index=False, encoding="utf-8", quoting=csv.QUOTE_ALL)
                print(f"    âœ… å·²ç”Ÿæˆ {student_id}_{file_index}.csv, å…± {len(group)} è¡Œ")
                file_index += 1
                continue
            
            # å°† LLM è¾“å‡ºçš„ session_id æ˜ å°„åˆ°å¯¹è¯
            session_ids = []
            prev_session = 1
            
            for idx in range(len(group)):
                key = str(idx + 1)  # è¡Œå·ä» 1 å¼€å§‹
                if key in mapping:
                    sid = mapping[key]
                else:
                    # å¦‚æœæ˜ å°„ä¸­æ²¡æœ‰è¿™ä¸ªé”®ï¼Œä½¿ç”¨å‰ä¸€ä¸ªsession id
                    print(f"    âš ï¸ æ˜ å°„ä¸­ç¼ºå°‘é”® '{key}'ï¼Œä½¿ç”¨å‰ä¸€ä¸ªsession ID")
                    sid = prev_session
                
                session_ids.append(sid)
                prev_session = sid
            
            # ç»Ÿè®¡sessionæ•°é‡
            unique_sessions = len(set(session_ids))
            print(f"    - LLMåˆ’åˆ†ä¸º {unique_sessions} ä¸ªå¯¹è¯")
            
            # æ ¹æ® session_id åˆ‡åˆ†å­ä¼šè¯
            current_session = []
            current_id = session_ids[0]
            
            for idx, sid in enumerate(session_ids):
                if sid != current_id:
                    # ä¿å­˜ä¸Šä¸€æ®µå­ä¼šè¯
                    if current_session:  # ç¡®ä¿ä¸ä¸ºç©º
                        sub_group = group.iloc[current_session].copy().reset_index(drop=True)
                        output_csv = os.path.join(output_folder, f"{student_id}_{file_index}.csv")
                        sub_group.to_csv(output_csv, index=False, encoding="utf-8", quoting=csv.QUOTE_ALL)
                        print(f"    âœ… å·²ç”Ÿæˆ {student_id}_{file_index}.csv, å…± {len(sub_group)} è¡Œ")
                        file_index += 1
                    
                    # å¼€å¯æ–°å­ä¼šè¯
                    current_session = [idx]
                    current_id = sid
                else:
                    current_session.append(idx)
            
            # ä¿å­˜æœ€åä¸€æ®µå­ä¼šè¯
            if current_session:
                sub_group = group.iloc[current_session].copy().reset_index(drop=True)
                output_csv = os.path.join(output_folder, f"{student_id}_{file_index}.csv")
                sub_group.to_csv(output_csv, index=False, encoding="utf-8", quoting=csv.QUOTE_ALL)
                print(f"    âœ… å·²ç”Ÿæˆ {student_id}_{file_index}.csv, å…± {len(sub_group)} è¡Œ")
                file_index += 1
    
    return file_index - 1  # è¿”å›ç”Ÿæˆçš„å¯¹è¯æ•°é‡

# ==============================
# å¤„ç†å•ä¸ª CSV æ–‡ä»¶
# ==============================
def process_csv_file(file_path):
    """å¤„ç†å•ä¸ªCSVæ–‡ä»¶ï¼ŒæŒ‰å­¦ç”ŸIDåˆ†ç»„å¹¶åˆ†åˆ«å¤„ç†"""
    try:
        # è·å–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        base_name = os.path.splitext(os.path.basename(file_path))[0]
        
        # åˆ›å»ºå¯¹åº”çš„è¾“å‡ºæ–‡ä»¶å¤¹
        output_folder = os.path.join(OUTPUT_BASE_FOLDER, base_name)
        os.makedirs(output_folder, exist_ok=True)
        
        print(f"\n{'='*60}")
        print(f"ğŸ“ å¼€å§‹å¤„ç†æ–‡ä»¶: {base_name}")
        print(f"ğŸ“‚ è¾“å‡ºæ–‡ä»¶å¤¹: {output_folder}")
        print(f"{'='*60}")
        
        # è¯»å– CSV
        df = robust_read_csv(file_path, text_columns=["æé—®å†…å®¹", "AIå›å¤"])
        if df is None or df.empty:
            print(f"âš ï¸ {file_path} æ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡")
            return
        
        # æŒ‰å­¦ç”ŸIDåˆ†ç»„
        student_groups = df.groupby("å­¦ç”ŸID")
        total_students = len(student_groups)
        
        print(f"ğŸ“Š å‘ç° {total_students} ä¸ªå­¦ç”Ÿçš„å¯¹è¯è®°å½•")
        
        total_dialogues = 0
        
        # å¤„ç†æ¯ä¸ªå­¦ç”Ÿçš„æ•°æ®
        for student_idx, (student_id, student_df) in enumerate(student_groups, start=1):
            print(f"\n[{student_idx}/{total_students}] å¤„ç†å­¦ç”Ÿ {student_id} çš„æ•°æ®...")
            print(f"  - æ€»è®°å½•æ•°: {len(student_df)}")
            
            # å¤„ç†è¯¥å­¦ç”Ÿçš„æ‰€æœ‰å¯¹è¯
            dialogue_count = process_student_data(student_df, student_id, output_folder)
            total_dialogues += dialogue_count
            
            print(f"  - ç”Ÿæˆå¯¹è¯æ•°: {dialogue_count}")
        
        print(f"\nâœ… æ–‡ä»¶ {base_name} å¤„ç†å®Œæˆï¼")
        print(f"  - å­¦ç”Ÿæ•°: {total_students}")
        print(f"  - æ€»å¯¹è¯æ•°: {total_dialogues}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶ {file_path} å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False

# ==============================
# ä¸»ç¨‹åº
# ==============================
def main():
    # è·å–æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = [f for f in os.listdir(INPUT_FOLDER) if f.endswith(".csv")]
    
    if not csv_files:
        print("âš ï¸ æœªæ‰¾åˆ°CSVæ–‡ä»¶")
        return
    
    print(f"ğŸ¯ æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶å¾…å¤„ç†")
    print(f"ğŸ“ è¾“å…¥æ–‡ä»¶å¤¹: {INPUT_FOLDER}")
    print(f"ğŸ“‚ è¾“å‡ºåŸºç¡€æ–‡ä»¶å¤¹: {OUTPUT_BASE_FOLDER}")
    print(f"{'='*60}\n")
    
    success_count = 0
    failed_files = []
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†æ–‡ä»¶
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = {
            executor.submit(process_csv_file, os.path.join(INPUT_FOLDER, f)): f 
            for f in csv_files
        }
        
        # ç­‰å¾…ä»»åŠ¡å®Œæˆ
        for future in as_completed(futures):
            file_name = futures[future]
            try:
                result = future.result()
                if result:
                    success_count += 1
                else:
                    failed_files.append(file_name)
            except Exception as e:
                print(f"âŒ å¤„ç† {file_name} å‡ºé”™: {e}")
                failed_files.append(file_name)
    
    # æ‰“å°æ€»ç»“
    print(f"\n{'='*60}")
    print(f"ğŸ‰ æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼")
    print(f"âœ… æˆåŠŸ: {success_count}/{len(csv_files)}")
    
    if failed_files:
        print(f"âŒ å¤±è´¥çš„æ–‡ä»¶:")
        for f in failed_files:
            print(f"  - {f}")
    
    print(f"{'='*60}")

if __name__ == "__main__":
    main()